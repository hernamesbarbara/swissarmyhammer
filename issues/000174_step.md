# Step 174: Performance Optimization and Caching

## Goal
Optimize the issue management system for performance, add intelligent caching, and implement monitoring to ensure the system scales well with large numbers of issues.

## Implementation Details

### 1. Add Issue Cache Layer
Create `swissarmyhammer/src/issues/cache.rs`:

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use tokio::time::{Duration, Instant};
use crate::issues::{Issue, Result};

#[derive(Debug, Clone)]
pub struct CacheEntry {
    pub issue: Issue,
    pub timestamp: Instant,
    pub access_count: u64,
}

/// In-memory cache for issue data with TTL and LRU eviction
pub struct IssueCache {
    entries: Arc<RwLock<HashMap<u32, CacheEntry>>>,
    ttl: Duration,
    max_size: usize,
    hits: Arc<RwLock<u64>>,
    misses: Arc<RwLock<u64>>,
}

impl IssueCache {
    pub fn new(ttl: Duration, max_size: usize) -> Self {
        Self {
            entries: Arc::new(RwLock::new(HashMap::new())),
            ttl,
            max_size,
            hits: Arc::new(RwLock::new(0)),
            misses: Arc::new(RwLock::new(0)),
        }
    }
    
    pub fn get(&self, issue_number: u32) -> Option<Issue> {
        let now = Instant::now();
        let mut entries = self.entries.write().unwrap();
        
        if let Some(entry) = entries.get_mut(&issue_number) {
            // Check if entry is still valid
            if now.duration_since(entry.timestamp) < self.ttl {
                entry.access_count += 1;
                entry.timestamp = now; // Update access time for LRU
                
                *self.hits.write().unwrap() += 1;
                return Some(entry.issue.clone());
            } else {
                // Entry expired, remove it
                entries.remove(&issue_number);
            }
        }
        
        *self.misses.write().unwrap() += 1;
        None
    }
    
    pub fn put(&self, issue: Issue) {
        let now = Instant::now();
        let mut entries = self.entries.write().unwrap();
        
        // Check if we need to evict entries
        if entries.len() >= self.max_size {
            self.evict_lru(&mut entries);
        }
        
        entries.insert(issue.number, CacheEntry {
            issue,
            timestamp: now,
            access_count: 1,
        });
    }
    
    pub fn invalidate(&self, issue_number: u32) {
        let mut entries = self.entries.write().unwrap();
        entries.remove(&issue_number);
    }
    
    pub fn clear(&self) {
        let mut entries = self.entries.write().unwrap();
        entries.clear();
    }
    
    pub fn stats(&self) -> CacheStats {
        let hits = *self.hits.read().unwrap();
        let misses = *self.misses.read().unwrap();
        let total = hits + misses;
        
        CacheStats {
            hits,
            misses,
            hit_rate: if total > 0 { hits as f64 / total as f64 } else { 0.0 },
            size: self.entries.read().unwrap().len(),
            max_size: self.max_size,
        }
    }
    
    fn evict_lru(&self, entries: &mut HashMap<u32, CacheEntry>) {
        if entries.is_empty() {
            return;
        }
        
        // Find the least recently used entry
        let lru_key = entries.iter()
            .min_by_key(|(_, entry)| entry.timestamp)
            .map(|(key, _)| *key)
            .unwrap();
        
        entries.remove(&lru_key);
    }
}

#[derive(Debug, Clone)]
pub struct CacheStats {
    pub hits: u64,
    pub misses: u64,
    pub hit_rate: f64,
    pub size: usize,
    pub max_size: usize,
}
```

### 2. Add Cached Issue Storage
Create `swissarmyhammer/src/issues/cached_storage.rs`:

```rust
use super::{Issue, IssueStorage, Result};
use super::cache::{IssueCache, CacheStats};
use async_trait::async_trait;
use tokio::time::Duration;
use std::sync::Arc;

pub struct CachedIssueStorage {
    storage: Box<dyn IssueStorage>,
    cache: Arc<IssueCache>,
}

impl CachedIssueStorage {
    pub fn new(storage: Box<dyn IssueStorage>) -> Self {
        let cache = Arc::new(IssueCache::new(
            Duration::from_secs(300), // 5 minutes TTL
            1000, // Max 1000 issues in cache
        ));
        
        Self { storage, cache }
    }
    
    pub fn with_cache_config(
        storage: Box<dyn IssueStorage>,
        ttl: Duration,
        max_size: usize,
    ) -> Self {
        let cache = Arc::new(IssueCache::new(ttl, max_size));
        Self { storage, cache }
    }
    
    pub fn cache_stats(&self) -> CacheStats {
        self.cache.stats()
    }
    
    pub fn clear_cache(&self) {
        self.cache.clear();
    }
}

#[async_trait]
impl IssueStorage for CachedIssueStorage {
    async fn create_issue(&self, name: String, content: String) -> Result<Issue> {
        let issue = self.storage.create_issue(name, content).await?;
        
        // Cache the new issue
        self.cache.put(issue.clone());
        
        Ok(issue)
    }
    
    async fn get_issue(&self, number: u32) -> Result<Issue> {
        // Try cache first
        if let Some(issue) = self.cache.get(number) {
            return Ok(issue);
        }
        
        // Cache miss, fetch from storage
        let issue = self.storage.get_issue(number).await?;
        
        // Cache the result
        self.cache.put(issue.clone());
        
        Ok(issue)
    }
    
    async fn update_issue(&self, number: u32, content: String) -> Result<Issue> {
        let issue = self.storage.update_issue(number, content).await?;
        
        // Update cache
        self.cache.put(issue.clone());
        
        Ok(issue)
    }
    
    async fn mark_complete(&self, number: u32) -> Result<Issue> {
        let issue = self.storage.mark_complete(number).await?;
        
        // Update cache
        self.cache.put(issue.clone());
        
        Ok(issue)
    }
    
    async fn list_issues(&self) -> Result<Vec<Issue>> {
        // For list operations, we typically don't cache the entire list
        // but we can cache individual issues from the list
        let issues = self.storage.list_issues().await?;
        
        // Cache individual issues
        for issue in &issues {
            self.cache.put(issue.clone());
        }
        
        Ok(issues)
    }
    
    async fn all_complete(&self) -> Result<bool> {
        // This is a computed property, don't cache it
        self.storage.all_complete().await
    }
}
```

### 3. Add Performance Monitoring
Create `swissarmyhammer/src/issues/metrics.rs`:

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};
use tokio::time::{Duration, Instant};

#[derive(Debug, Clone)]
pub struct PerformanceMetrics {
    pub operation_counts: Arc<OperationCounts>,
    pub timing_stats: Arc<TimingStats>,
}

#[derive(Debug)]
pub struct OperationCounts {
    pub create_operations: AtomicU64,
    pub read_operations: AtomicU64,
    pub update_operations: AtomicU64,
    pub delete_operations: AtomicU64,
    pub list_operations: AtomicU64,
}

#[derive(Debug)]
pub struct TimingStats {
    pub total_create_time: AtomicU64,
    pub total_read_time: AtomicU64,
    pub total_update_time: AtomicU64,
    pub total_delete_time: AtomicU64,
    pub total_list_time: AtomicU64,
}

impl PerformanceMetrics {
    pub fn new() -> Self {
        Self {
            operation_counts: Arc::new(OperationCounts {
                create_operations: AtomicU64::new(0),
                read_operations: AtomicU64::new(0),
                update_operations: AtomicU64::new(0),
                delete_operations: AtomicU64::new(0),
                list_operations: AtomicU64::new(0),
            }),
            timing_stats: Arc::new(TimingStats {
                total_create_time: AtomicU64::new(0),
                total_read_time: AtomicU64::new(0),
                total_update_time: AtomicU64::new(0),
                total_delete_time: AtomicU64::new(0),
                total_list_time: AtomicU64::new(0),
            }),
        }
    }
    
    pub fn record_operation(&self, operation: Operation, duration: Duration) {
        let duration_micros = duration.as_micros() as u64;
        
        match operation {
            Operation::Create => {
                self.operation_counts.create_operations.fetch_add(1, Ordering::Relaxed);
                self.timing_stats.total_create_time.fetch_add(duration_micros, Ordering::Relaxed);
            }
            Operation::Read => {
                self.operation_counts.read_operations.fetch_add(1, Ordering::Relaxed);
                self.timing_stats.total_read_time.fetch_add(duration_micros, Ordering::Relaxed);
            }
            Operation::Update => {
                self.operation_counts.update_operations.fetch_add(1, Ordering::Relaxed);
                self.timing_stats.total_update_time.fetch_add(duration_micros, Ordering::Relaxed);
            }
            Operation::Delete => {
                self.operation_counts.delete_operations.fetch_add(1, Ordering::Relaxed);
                self.timing_stats.total_delete_time.fetch_add(duration_micros, Ordering::Relaxed);
            }
            Operation::List => {
                self.operation_counts.list_operations.fetch_add(1, Ordering::Relaxed);
                self.timing_stats.total_list_time.fetch_add(duration_micros, Ordering::Relaxed);
            }
        }
    }
    
    pub fn get_stats(&self) -> MetricsSnapshot {
        MetricsSnapshot {
            create_ops: self.operation_counts.create_operations.load(Ordering::Relaxed),
            read_ops: self.operation_counts.read_operations.load(Ordering::Relaxed),
            update_ops: self.operation_counts.update_operations.load(Ordering::Relaxed),
            delete_ops: self.operation_counts.delete_operations.load(Ordering::Relaxed),
            list_ops: self.operation_counts.list_operations.load(Ordering::Relaxed),
            
            avg_create_time: self.calculate_avg_time(
                self.timing_stats.total_create_time.load(Ordering::Relaxed),
                self.operation_counts.create_operations.load(Ordering::Relaxed)
            ),
            avg_read_time: self.calculate_avg_time(
                self.timing_stats.total_read_time.load(Ordering::Relaxed),
                self.operation_counts.read_operations.load(Ordering::Relaxed)
            ),
            avg_update_time: self.calculate_avg_time(
                self.timing_stats.total_update_time.load(Ordering::Relaxed),
                self.operation_counts.update_operations.load(Ordering::Relaxed)
            ),
            avg_delete_time: self.calculate_avg_time(
                self.timing_stats.total_delete_time.load(Ordering::Relaxed),
                self.operation_counts.delete_operations.load(Ordering::Relaxed)
            ),
            avg_list_time: self.calculate_avg_time(
                self.timing_stats.total_list_time.load(Ordering::Relaxed),
                self.operation_counts.list_operations.load(Ordering::Relaxed)
            ),
        }
    }
    
    fn calculate_avg_time(&self, total_time: u64, count: u64) -> f64 {
        if count == 0 {
            0.0
        } else {
            total_time as f64 / count as f64
        }
    }
    
    pub fn reset(&self) {
        self.operation_counts.create_operations.store(0, Ordering::Relaxed);
        self.operation_counts.read_operations.store(0, Ordering::Relaxed);
        self.operation_counts.update_operations.store(0, Ordering::Relaxed);
        self.operation_counts.delete_operations.store(0, Ordering::Relaxed);
        self.operation_counts.list_operations.store(0, Ordering::Relaxed);
        
        self.timing_stats.total_create_time.store(0, Ordering::Relaxed);
        self.timing_stats.total_read_time.store(0, Ordering::Relaxed);
        self.timing_stats.total_update_time.store(0, Ordering::Relaxed);
        self.timing_stats.total_delete_time.store(0, Ordering::Relaxed);
        self.timing_stats.total_list_time.store(0, Ordering::Relaxed);
    }
}

#[derive(Debug, Clone)]
pub enum Operation {
    Create,
    Read,
    Update,
    Delete,
    List,
}

#[derive(Debug, Clone)]
pub struct MetricsSnapshot {
    pub create_ops: u64,
    pub read_ops: u64,
    pub update_ops: u64,
    pub delete_ops: u64,
    pub list_ops: u64,
    
    pub avg_create_time: f64,
    pub avg_read_time: f64,
    pub avg_update_time: f64,
    pub avg_delete_time: f64,
    pub avg_list_time: f64,
}

impl MetricsSnapshot {
    pub fn total_operations(&self) -> u64 {
        self.create_ops + self.read_ops + self.update_ops + self.delete_ops + self.list_ops
    }
    
    pub fn overall_avg_time(&self) -> f64 {
        let total_time = (self.create_ops as f64 * self.avg_create_time)
            + (self.read_ops as f64 * self.avg_read_time)
            + (self.update_ops as f64 * self.avg_update_time)
            + (self.delete_ops as f64 * self.avg_delete_time)
            + (self.list_ops as f64 * self.avg_list_time);
        
        let total_ops = self.total_operations();
        if total_ops == 0 {
            0.0
        } else {
            total_time / total_ops as f64
        }
    }
}
```

### 4. Add Instrumented Storage Wrapper
Create `swissarmyhammer/src/issues/instrumented_storage.rs`:

```rust
use super::{Issue, IssueStorage, Result};
use super::metrics::{PerformanceMetrics, Operation};
use async_trait::async_trait;
use tokio::time::Instant;

pub struct InstrumentedIssueStorage {
    storage: Box<dyn IssueStorage>,
    metrics: PerformanceMetrics,
}

impl InstrumentedIssueStorage {
    pub fn new(storage: Box<dyn IssueStorage>) -> Self {
        Self {
            storage,
            metrics: PerformanceMetrics::new(),
        }
    }
    
    pub fn metrics(&self) -> &PerformanceMetrics {
        &self.metrics
    }
}

#[async_trait]
impl IssueStorage for InstrumentedIssueStorage {
    async fn create_issue(&self, name: String, content: String) -> Result<Issue> {
        let start = Instant::now();
        let result = self.storage.create_issue(name, content).await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::Create, duration);
        result
    }
    
    async fn get_issue(&self, number: u32) -> Result<Issue> {
        let start = Instant::now();
        let result = self.storage.get_issue(number).await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::Read, duration);
        result
    }
    
    async fn update_issue(&self, number: u32, content: String) -> Result<Issue> {
        let start = Instant::now();
        let result = self.storage.update_issue(number, content).await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::Update, duration);
        result
    }
    
    async fn mark_complete(&self, number: u32) -> Result<Issue> {
        let start = Instant::now();
        let result = self.storage.mark_complete(number).await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::Delete, duration);
        result
    }
    
    async fn list_issues(&self) -> Result<Vec<Issue>> {
        let start = Instant::now();
        let result = self.storage.list_issues().await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::List, duration);
        result
    }
    
    async fn all_complete(&self) -> Result<bool> {
        let start = Instant::now();
        let result = self.storage.all_complete().await;
        let duration = start.elapsed();
        
        self.metrics.record_operation(Operation::List, duration);
        result
    }
}
```

### 5. Add Batch Operations
Add batch operation support to `swissarmyhammer/src/issues/mod.rs`:

```rust
use async_trait::async_trait;

#[async_trait]
pub trait BatchIssueStorage: IssueStorage {
    /// Create multiple issues in a single operation
    async fn create_issues_batch(&self, issues: Vec<(String, String)>) -> Result<Vec<Issue>>;
    
    /// Get multiple issues in a single operation
    async fn get_issues_batch(&self, numbers: Vec<u32>) -> Result<Vec<Issue>>;
    
    /// Update multiple issues in a single operation
    async fn update_issues_batch(&self, updates: Vec<(u32, String)>) -> Result<Vec<Issue>>;
    
    /// Mark multiple issues complete in a single operation
    async fn mark_complete_batch(&self, numbers: Vec<u32>) -> Result<Vec<Issue>>;
}

#[async_trait]
impl BatchIssueStorage for FileSystemIssueStorage {
    async fn create_issues_batch(&self, issues: Vec<(String, String)>) -> Result<Vec<Issue>> {
        let mut results = Vec::new();
        
        for (name, content) in issues {
            let issue = self.create_issue(name, content).await?;
            results.push(issue);
        }
        
        Ok(results)
    }
    
    async fn get_issues_batch(&self, numbers: Vec<u32>) -> Result<Vec<Issue>> {
        let mut results = Vec::new();
        
        for number in numbers {
            let issue = self.get_issue(number).await?;
            results.push(issue);
        }
        
        Ok(results)
    }
    
    async fn update_issues_batch(&self, updates: Vec<(u32, String)>) -> Result<Vec<Issue>> {
        let mut results = Vec::new();
        
        for (number, content) in updates {
            let issue = self.update_issue(number, content).await?;
            results.push(issue);
        }
        
        Ok(results)
    }
    
    async fn mark_complete_batch(&self, numbers: Vec<u32>) -> Result<Vec<Issue>> {
        let mut results = Vec::new();
        
        for number in numbers {
            let issue = self.mark_complete(number).await?;
            results.push(issue);
        }
        
        Ok(results)
    }
}
```

### 6. Add Performance Benchmarks
Create `swissarmyhammer/benches/issue_performance.rs`:

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use swissarmyhammer::issues::{FileSystemIssueStorage, IssueStorage};
use tempfile::TempDir;
use tokio::runtime::Runtime;

fn bench_issue_operations(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    c.bench_function("create_issue", |b| {
        b.iter(|| {
            rt.block_on(async {
                let temp_dir = TempDir::new().unwrap();
                std::env::set_current_dir(temp_dir.path()).unwrap();
                
                let storage = FileSystemIssueStorage::new().unwrap();
                let issue = storage.create_issue(
                    black_box("benchmark_issue".to_string()),
                    black_box("Benchmark content".to_string()),
                ).await.unwrap();
                
                black_box(issue);
            })
        })
    });
    
    c.bench_function("get_issue", |b| {
        let temp_dir = TempDir::new().unwrap();
        std::env::set_current_dir(temp_dir.path()).unwrap();
        
        let storage = FileSystemIssueStorage::new().unwrap();
        let issue = rt.block_on(storage.create_issue(
            "benchmark_issue".to_string(),
            "Benchmark content".to_string(),
        )).unwrap();
        
        b.iter(|| {
            rt.block_on(async {
                let retrieved = storage.get_issue(black_box(issue.number)).await.unwrap();
                black_box(retrieved);
            })
        })
    });
    
    c.bench_function("list_issues_100", |b| {
        let temp_dir = TempDir::new().unwrap();
        std::env::set_current_dir(temp_dir.path()).unwrap();
        
        let storage = FileSystemIssueStorage::new().unwrap();
        
        // Create 100 issues
        for i in 1..=100 {
            rt.block_on(storage.create_issue(
                format!("issue_{}", i),
                format!("Content for issue {}", i),
            )).unwrap();
        }
        
        b.iter(|| {
            rt.block_on(async {
                let issues = storage.list_issues().await.unwrap();
                black_box(issues);
            })
        })
    });
}

criterion_group!(benches, bench_issue_operations);
criterion_main!(benches);
```

## Testing
- Test cache hit/miss ratios and performance improvements
- Test metrics collection accuracy
- Test batch operations efficiency
- Benchmark performance with various issue counts
- Test memory usage and cache eviction
- Test concurrent access patterns

## Success Criteria
- Cache improves read performance by at least 50%
- Metrics provide accurate performance insights
- Batch operations reduce overhead for bulk operations
- Memory usage remains reasonable under load
- Performance scales well with issue count
- Cache eviction prevents memory leaks